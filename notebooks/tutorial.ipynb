{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Examples on FAUST using Chebyshev Convolutions\n",
    "First, we need to import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# built-in libraries\n",
    "import sys\n",
    "import os \n",
    "\n",
    "# third party libraries\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch \n",
    "import torch.nn.functional as func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test data is loaded in memory using the `dataset` local module. \n",
    "To use the FAUST dataset, you need to download it from [here](http://faust.is.tue.mpg.de/) and place it in the directory `{repository-root}/datasets/faust/raw/`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giorg\\Anaconda3\\envs\\adex\\lib\\site-packages\\torch_geometric\\data\\dataset.py:94: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you really want to make use of another pre-processing technique, make sure to delete `C:\\Users\\giorg\\Adversarial-Examples-on-Meshes\\datasets\\faust\\processed/processed` first.\n",
      "  self.processed_dir))\n"
     ]
    }
   ],
   "source": [
    "REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(os.path.realpath('__file__')),\"..\"))\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "SRC_DIR = os.path.join(REPO_ROOT,\"src\")\n",
    "FAUST = os.path.join(REPO_ROOT,\"datasets/faust\")\n",
    "PARAMS_FILE = os.path.join(REPO_ROOT, \"model_data/FAUST10.pt\")\n",
    "\n",
    "# repository modules\n",
    "sys.path.insert(0, SRC_DIR)\n",
    "import models\n",
    "import train\n",
    "import dataset\n",
    "import utils\n",
    "\n",
    "traindata = dataset.FaustDataset(FAUST, device=DEVICE, train=True, test=False, transform_data=True)\n",
    "testdata = dataset.FaustDataset(FAUST, device=DEVICE, train=False, test=True,  transform_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define the classifier for the human pose classefication task; download the classifier parameters from [here](https://drive.google.com/drive/folders/1Bv0QM6R06nyCr9J-5sGIEtLKwxM6Qowa?usp=sharing). Move these parameters in `{repository-root}/model_data/FAUST10.pt` .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChebnetClassifier(\n",
      "  (chebconv_0): ChebConv(3, 128, K=6, normalization=sym)\n",
      "  (chebconv_1): ChebConv(128, 128, K=6, normalization=sym)\n",
      "  (chebconv_2): ChebConv(128, 64, K=6, normalization=sym)\n",
      "  (chebconv_3): ChebConv(64, 64, K=6, normalization=sym)\n",
      "  (linear): Linear(in_features=6912, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.ChebnetClassifier(\n",
    "    param_conv_layers=[128,128,64,64],\n",
    "    D_t = traindata.downscale_matrices,\n",
    "    E_t = traindata.downscaled_edges,\n",
    "    num_classes = traindata.num_classes,\n",
    "    parameters_file=PARAMS_FILE).to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and evaluation can be done through the `train` local module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:07<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train network\n",
    "train.train(\n",
    "    train_data=traindata,\n",
    "    classifier=model,\n",
    "    parameters_file=PARAMS_FILE,\n",
    "    epoch_number=0) # <- change here the number of epochs used for training\n",
    "\n",
    "#compute accuracy\n",
    "accuracy, confusion_matrix = train.evaluate(eval_data=testdata, classifier=model, epoch_number=1)\n",
    "\n",
    "print(accuracy)\n",
    "plt.matshow(confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for Targeted Adversarial Examples with C&W Approach\n",
    "This part of the notebook shows how to search for adversarial examples using our version of the  *Carlini \\& Wagner* approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adversarial.carlini_wagner as cw\n",
    "from adversarial.carlini_wagner import CWBuilder, LowbandPerturbation, generate_adversarial_example\n",
    "\n",
    "# get a random mesh from the dataset and a random target class\n",
    "import random\n",
    "while True:\n",
    "    i = random.randint(0, len(testdata)-1)\n",
    "    t = random.randint(0, testdata.num_classes-1)\n",
    "    y = testdata[i].y.item()\n",
    "    if y != t: break\n",
    "mesh = testdata[i]\n",
    "\n",
    "#parameters used to search the adversarial example\n",
    "params = {\n",
    "    CWBuilder.USETQDM:True,\n",
    "    CWBuilder.MIN_IT:1000,      #<- number of iterations for the adversarial example computation (common value: 1000)\n",
    "    CWBuilder.LEARN_RATE:1e-4, #<- learning rate used during optimization \n",
    "    CWBuilder.ADV_COEFF:1e-2, #<- starting coefficient applied to the adversarial loss (updated via exponential search)\n",
    "    CWBuilder.REG_COEFF:1, #<- coefficient applied to the regularization term\n",
    "    LowbandPerturbation.EIGS_NUMBER:40 #<- number of eigenvalues used for the low-band perturbation\n",
    "    }  \n",
    "\n",
    "\n",
    "# configure adversarial example components using builder\n",
    "#--------------------------------------------------------\n",
    "builder = CWBuilder(search_iterations=1)\n",
    "builder.set_classifier(model)\n",
    "builder.set_mesh(pos=mesh.pos, edges=mesh.edge_index.t(), faces=mesh.face.t())\n",
    "builder.set_target(t)\n",
    "builder.set_similarity_loss(cw.LocalEuclideanSimilarity)\n",
    "builder.set_adversarial_loss(cw.AdversarialLoss)\n",
    "builder.set_regularization_loss(cw.EmptyRegularizer) #<- i.e. no regularization term\n",
    "adex = builder.build(**params)\n",
    "print(\"adversarial attack: \"+(\"successful\" if adex.is_successful else \"unsuccessful\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally see the actual adversarial example using **Plotly** (note: you need to install plotly before-hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def visualize(pos, faces, intensity=None):\n",
    "  cpu = torch.device(\"cpu\")\n",
    "  if type(pos) != np.ndarray:\n",
    "    pos = pos.to(cpu).clone().detach().numpy()\n",
    "  if pos.shape[-1] != 3:\n",
    "    raise ValueError(\"Vertices positions must have shape [n,3]\")\n",
    "  if type(faces) != np.ndarray:\n",
    "    faces = faces.to(cpu).clone().detach().numpy()\n",
    "  if faces.shape[-1] != 3:\n",
    "    raise ValueError(\"Face indices must have shape [m,3]\") \n",
    "  if intensity is None:\n",
    "    intensity = np.ones([pos.shape[0]])\n",
    "  elif type(intensity) != np.ndarray:\n",
    "    intensity = intensity.to(cpu).clone().detach().numpy()\n",
    "\n",
    "  x, z, y = pos.T\n",
    "  i, j, k = faces.T\n",
    "\n",
    "  mesh = go.Mesh3d(x=x, y=y, z=z,\n",
    "            color='lightpink',\n",
    "            intensity=intensity,\n",
    "            opacity=1,\n",
    "            colorscale=[[0, 'gold'],[0.5, 'mediumturquoise'],[1, 'magenta']],\n",
    "            i=i, j=j, k=k,\n",
    "            showscale=True)\n",
    "  layout = go.Layout(scene=go.layout.Scene(aspectmode=\"data\")) \n",
    "\n",
    "  #pio.renderers.default=\"plotly_mimetype\"\n",
    "  fig = go.Figure(data=[mesh],\n",
    "                  layout=layout)\n",
    "  fig.update_layout(\n",
    "      autosize=True,\n",
    "      margin=dict(l=20, r=20, t=20, b=20),\n",
    "      paper_bgcolor=\"LightSteelBlue\")\n",
    "  fig.show()\n",
    "    \n",
    "def compare(pos1, faces1, pos2, faces2):\n",
    "    n,m = pos1.shape[0], pos2.shape[0]\n",
    "    tmpx = torch.cat([pos1, pos2],dim=0)\n",
    "    tmpf = torch.cat([faces1, faces2+n], dim=0)\n",
    "    color = torch.zeros([n+m],dtype=pos1.dtype, device=pos1.device)\n",
    "    color[n:] = (pos1-pos2).norm(p=2,dim=-1)\n",
    "    visualize(tmpx, tmpf,color)\n",
    "    \n",
    "def show_logits(logits):\n",
    "  logits = logits.cpu().detach().numpy().squeeze()\n",
    "  m = min([logits.min()])\n",
    "  num_classes = logits.shape[0]\n",
    "  \n",
    "  x_ticks = np.array(range(num_classes),dtype=float)\n",
    "  ax = plt.subplot(111)\n",
    "  ax.bar(x_ticks-0.2, logits-m, width=0.4, color='b', align='center')\n",
    "  ax.legend([\"logits\"])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(adex.perturbed_pos, f, (adex.pos-adex.perturbed_pos).norm(p=2,dim=-1))\n",
    "compare(adex.pos, adex.faces, adex.perturbed_pos, adex.faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Untargeted (FSGM and related methods)\n",
    "Here instead is shown how to find adversarial examples using our iterative FSGM variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adversarial.pgd import PGDBuilder,LowBandTransform, ClipTransform, generate_adversarial_example\n",
    "import random\n",
    "i = random.randint(0, len(testdata)-1)\n",
    "mesh = testdata[i]\n",
    "\n",
    "params = {PGDBuilder.IT:10,  #<- number of iterations\n",
    "         LowBandTransform.EIGS_NUMBER:40, #<- number of eigenvalues to use for the low-band filtering\n",
    "         ClipTransform.EPSILON:1} #<- epsilon value (used for clipping pertrbation of vertices)\n",
    "\n",
    "adex = generate_adversarial_example(\n",
    "    mesh, model, alpha=0.002, **params, \n",
    "    clip_transform=\"pointwise\", \n",
    "    lowband_transform=\"static\", \n",
    "    gradient_transform=\"sign\")\n",
    "\n",
    "visualize(adex.perturbed_pos, f, (adex.pos -adex.perturbed_pos).norm(p=2,dim=-1))\n",
    "print(\"adversarial attack: \"+(\"successful\" if adex.is_successful else \"unsuccessful\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Examples on SMAL\n",
    "SMAL is handled in a similar fashion to FAUST. The pre-trained parameters can be found in the file `SMAL.pt` from [here](https://drive.google.com/drive/folders/1Bv0QM6R06nyCr9J-5sGIEtLKwxM6Qowa?usp=sharing). Move this file in `{repository-root}/model_data/SMAL.pt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMAL = os.path.join(REPO_ROOT,\"datasets/smal\")\n",
    "SMAL_PARAMETERS = os.path.join(REPO_ROOT, \"model_data/SMAL.pt\")\n",
    "traindata = dataset.SmalDataset(SMAL, device=DEVICE, train=True, test=False,transform_data=True)\n",
    "testdata = dataset.SmalDataset(SMAL, device=DEVICE, train=False, test=True,transform_data=False)\n",
    "\n",
    "model = models.ChebnetClassifier(\n",
    "    param_conv_layers=[128,128,64,64],\n",
    "    D_t = traindata.downscale_matrices,\n",
    "    E_t = traindata.downscaled_edges,\n",
    "    num_classes = traindata.num_classes,\n",
    "    parameters_file=SMAL_PARAMETERS).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute accuracy\n",
    "accuracy, confusion_matrix = train.evaluate(eval_data=testdata, classifier=model, epoch_number=1)\n",
    "print(accuracy)\n",
    "plt.matshow(confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the adversarial surfaces search is similar to FAUST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adversarial.carlini_wagner as cw\n",
    "from adversarial.carlini_wagner import CWBuilder, generate_adversarial_example\n",
    "\n",
    "# get a random mesh from the dataset and a random target class\n",
    "import random\n",
    "while True:\n",
    "    i = random.randint(0, len(testdata)-1)\n",
    "    t = random.randint(0, testdata.num_classes-1)\n",
    "    y = testdata[i].y.item()\n",
    "    if y != t: break\n",
    "mesh = testdata[i]\n",
    "\n",
    "#parameters used to search the adversarial example\n",
    "params = {\n",
    "    CWBuilder.USETQDM:True,\n",
    "    CWBuilder.MIN_IT:1000,      #<- number of iterations for the adversarial example computation (common value: 1000)\n",
    "    CWBuilder.LEARN_RATE:1e-4, #<- learning rate used during optimization \n",
    "    CWBuilder.ADV_COEFF:1e-2, #<- starting coefficient applied to the adversarial loss (updated via exponential search)\n",
    "    CWBuilder.REG_COEFF:1, #<- coefficient applied to the regularization term\n",
    "    LowbandPerturbation.EIGS_NUMBER:40 #<- number of eigenvalues used for the low-band perturbation\n",
    "    }  \n",
    "\n",
    "# configure adversarial example using 'generate_adversarial_example'\n",
    "#-------------------------------------------------------------------\n",
    "adex = generate_adversarial_example(search_iterations=7, mesh=mesh, classifier=model, target=t, **params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
